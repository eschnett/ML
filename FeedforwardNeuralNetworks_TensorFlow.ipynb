{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FeedforwardNeuralNetworks_TensorFlow.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eWxxmBLYKoln"},"source":["# Feedforward neural networks in TensorFlow\n","\n","Perimeter Institute Computational Tutorial Series\n","\n","October 29, 2019\n","\n","Lauren Hayward "]},{"cell_type":"markdown","metadata":{"id":"MuUm4oZtGM77","colab_type":"text"},"source":["The objective of this tutorial is to become comfortable with using the software library TensorFlow to \n","create and train a simple feedforward neural network for supervised learning.\n","\n","Let us start by generating a random dataset of two-dimensional points with $K$ branches. For each datapoint $\\mathbf{x} = (x_1, x_2)$, the label is the branch index such that $y = 0, 1, \\ldots K-2$ or $K-1$.\n","Our goal is to implement a neural network capable of classifying the branches.\n"]},{"cell_type":"code","metadata":{"id":"ZD4zSMIhdITs","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","############################################################################\n","####################### CREATE AND PLOT THE DATA SET #######################\n","############################################################################\n","\n","N = 50 # number of points per branch\n","K = 3  # number of branches\n","\n","N_train = N*K # total number of points in the training set\n","x_train = np.zeros((N_train,2)) # matrix containing the 2-dimensional datapoints\n","y_train = np.zeros(N_train, dtype='uint8') # labels (not in one-hot representation)\n","\n","mag_noise = 0.3  # controls how much noise gets added to the data\n","dTheta    = 4    # difference in theta in each branch\n","\n","### Data generation: ###\n","for j in range(K):\n","  ix = range(N*j,N*(j+1))\n","  r = np.linspace(0.01,1,N) # radius\n","  t = np.linspace(j*(2*np.pi)/K,j*(2*np.pi)/K + dTheta,N) + np.random.randn(N)*mag_noise # theta\n","  x_train[ix] = np.c_[r*np.cos(t), r*np.sin(t)]\n","  y_train[ix] = j\n","\n","### Plot the data set: ###\n","fig = plt.figure(1, figsize=(5,5))\n","plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=40)#, cmap=plt.cm.Spectral)\n","plt.xlim([-1,1])\n","plt.ylim([-1,1])\n","plt.xlabel(r'$x_1$')\n","plt.ylabel(r'$x_2$')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaSSK_89e0Js","colab_type":"text"},"source":["This network will compare its output with labels in the so-called *one-hot encoding*.\n","For a given label $y=k$, the corresponding one-hot encoding is a $K$-dimensional vector with all entries zero \n","except for the $k^\\text{th}$ entry (which has value 1).\n","So, for example, when $K=3$ the one-hot encodings for the labels are\n","\\begin{equation*}\n","0 \\rightarrow \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\qquad\n","1 \\rightarrow \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\qquad\n","2 \\rightarrow \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n","\\end{equation*}"]},{"cell_type":"markdown","metadata":{"id":"sIz8oa2YQJr_","colab_type":"text"},"source":["**Exercise #1:** Run the code below, which first defines the structure of the neural network \n","and then uses the dataset to train this network. \n","Look at how this code attempts to classify the two-dimensional space.\n","You should find that the resulting classifier separates the two-dimensional space using lines, \n","and thus does a poor job of representing the data."]},{"cell_type":"code","metadata":{"id":"EG5DlljlSEvB","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","from IPython import display\n","import tensorflow as tf\n","import time\n","\n","############################################################################\n","##################### DEFINE THE NETWORK ARCHITECTURE ######################\n","############################################################################\n","\n","### Create placeholders for the input data and labels ###\n","### (we'll input actual values when we ask TensorFlow to run an actual computation later) ###\n","x = tf.placeholder(tf.float32, [None, 2]) # input data\n","y = tf.placeholder(tf.int32,[None])       # labels\n","\n","### Layer 1: ###\n","W1 = tf.Variable( tf.random_normal([2, K], mean=0.0, stddev=0.01, dtype=tf.float32) )\n","b1 = tf.Variable( tf.zeros([K]) )\n","z1 = tf.matmul(x, W1) + b1\n","a1 = tf.nn.sigmoid( z1 )\n","\n","### Network output: ###\n","aL = a1\n","\n","### Cost function: ###\n","### (measures how far off our model is from the labels) ###\n","y_onehot = tf.one_hot(y,depth=K) # labels are converted to one-hot representation\n","eps=0.0000000001 # to prevent the logs from diverging\n","cross_entropy = tf.reduce_mean(-tf.reduce_sum( y_onehot * tf.log(aL+eps) +  (1.0-y_onehot )*tf.log(1.0-aL +eps) , reduction_indices=[1]))\n","cost_func = cross_entropy\n","\n","### Use backpropagation to minimize the cost function using the gradient descent algorithm: ###\n","learning_rate  = 1.0 # hyperparameter\n","train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_func)\n","\n","N_epochs = 20000 # number of times to run gradient descent\n","\n","##############################################################################\n","################################## TRAINING ##################################\n","##############################################################################\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","epoch_list    = []\n","cost_training = []\n","acc_training  = []\n","\n","############ Function for plotting: ############\n","def updatePlot():\n","\n","    ### Generate coordinates covering the whole plane: ###\n","    padding = 0.1\n","    spacing = 0.02\n","    x1_min, x1_max = x_train[:, 0].min() - padding, x_train[:, 0].max() + padding\n","    x2_min, x2_max = x_train[:, 1].min() - padding, x_train[:, 1].max() + padding\n","    x1_grid, x2_grid = np.meshgrid(np.arange(x1_min, x1_max, spacing),\n","                         np.arange(x2_min, x2_max, spacing))\n","\n","    NN_output       = sess.run(aL,feed_dict={x:np.c_[x1_grid.ravel(), x2_grid.ravel()]})\n","    predicted_class = np.argmax(NN_output, axis=1)\n","\n","    ### Plot the classifier: ###\n","    plt.subplot(121)\n","    plt.contourf(x1_grid, x2_grid, predicted_class.reshape(x1_grid.shape), K, alpha=0.8)\n","    plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=40)\n","    plt.xlim(x1_grid.min(), x1_grid.max())\n","    plt.ylim(x2_grid.min(), x2_grid.max())\n","    plt.xlabel(r'$x_1$')\n","    plt.ylabel(r'$x_2$')\n","\n","    ### Plot the cost function during training: ###\n","    plt.subplot(222)\n","    plt.plot(epoch_list,cost_training,'o-')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Training cost')\n","\n","    ### Plot the training accuracy: ###\n","    plt.subplot(224)\n","    plt.plot(epoch_list,acc_training,'o-')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Training accuracy')\n","############ End of plotting function ############\n","\n","### Train for several epochs: ###\n","for epoch in range(N_epochs):\n","    sess.run(train_step, feed_dict={x: x_train,y:y_train}) #run gradient descent\n","    \n","    ### Update the plot and print results every 500 epochs: ###\n","    if epoch % 500 == 0:\n","        cost = sess.run(cost_func,feed_dict={x:x_train, y:y_train})\n","        NN_output = sess.run(aL,feed_dict={x:x_train, y:y_train})\n","        predicted_class = np.argmax(NN_output, axis=1)\n","        accuracy = np.mean(predicted_class == y_train)\n","    \n","        #print( \"Iteration %d:\\n  Training cost %f\\n  Training accuracy %f\\n\" % (epoch, cost, accuracy) )\n","    \n","        epoch_list.append(epoch)\n","        cost_training.append(cost)\n","        acc_training.append(accuracy)\n","        \n","        ### Update the plot of the resulting classifier: ###\n","        fig = plt.figure(2,figsize=(10,5))\n","        fig.subplots_adjust(hspace=.3,wspace=.3)\n","        plt.clf()\n","        updatePlot()\n","        display.display(plt.gcf())\n","        display.clear_output(wait=True)\n","        #time.sleep(0.1) #Uncomment this line if you want to slow down the rate of plot updates"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5RkRepmwYQG","colab_type":"text"},"source":["**Exercise #2:** Look through the section of code marked `DEFINE THE NETWORK ARCHITECTURE`.\n","On paper, draw the neural network corresponding to the one in the code for the case of $K$ branches. \n","Pay particular attention to the number of neurons in each layer.\n","\n","\n","**Exercise #3:** Add in a hidden layer with 4 neurons and study how this hidden layer changes the output. \n","On paper, draw the neural network in this case.\n","\n","**Exercise #4:** Replace the sigmoid activation function on the first layer with a rectified linear unit (ReLU), and study how the \n","choice of activation function changes the output.\n","\n","**Exercise #5:** Change the cost function so that it is computed using the mean-squared error (MSE) instead of the cross-entropy,\n","and study how the choice of cost function changes the output.\n","\n","**Exercise #6:** Study the effects of increasing and decreasing the `learning_rate` hyperparameter.\n","Examine these effects using both the cross-entropy and mean-squared error cost functions.\n","\n","**Exercise #7:** Explain why the $K$-dimensional one-hot encoding is useful. What do you think would happen if you used a one-dimensional label (such that $y=0,1,\\ldots, K-1$ or $K$) instead?\n","\n","**Exercise #8:** Study how the neural network's accurary changes as a function of:\n","\n","\n","*   the number of neurons in the hidden layer\n","*   `mag_noise` (the magnitude of noise in the data)\n","*   the number of different labels `K`\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"EH6sqQuQRGNz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}